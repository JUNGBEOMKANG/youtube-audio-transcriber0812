"""
YouTube Audio Transcriber ì›¹ ì• í”Œë¦¬ì¼€ì´ì…˜
"""
from fastapi import FastAPI, Form, HTTPException, BackgroundTasks, Request
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from fastapi.templating import Jinja2Templates
import uvicorn
import os
from audio_extractor import YouTubeAudioExtractor
from speech_transcriber import SpeechTranscriber
import uuid
from datetime import datetime
from transformers import pipeline
from pydantic import BaseModel
import torch
from dotenv import load_dotenv
import google.generativeai as genai
import asyncio
import json

# --- Environment Setup ---
load_dotenv()

# Google Gemini ì„¤ì •
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY")
GEMINI_MODEL = os.getenv("GEMINI_MODEL", "gemini-1.5-flash")
USE_GEMINI_SUMMARY = os.getenv("USE_GEMINI_SUMMARY", "false").lower() == "true"

if GEMINI_API_KEY:
    genai.configure(api_key=GEMINI_API_KEY)
    print(f"ğŸ”‘ Google Gemini API ì„¤ì • ì™„ë£Œ! (ëª¨ë¸: {GEMINI_MODEL})")
else:
    print("âš ï¸  Gemini API í‚¤ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ì¡´ T5 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")

# --- App Initialization ---
app = FastAPI(
    title="YouTube Audio Transcriber", 
    version="3.1.0",
    description="ê³ í’ˆì§ˆ AIë¥¼ ì‚¬ìš©í•œ YouTube ìŒì„±-í…ìŠ¤íŠ¸ ë³€í™˜ ë° ìš”ì•½ ì„œë¹„ìŠ¤ (OpenAI í†µí•©)",
    docs_url="/api/docs",
    redoc_url="/api/redoc"
)

# --- Static Files and Templates ---
app.mount("/static", StaticFiles(directory="static"), name="static")
templates = Jinja2Templates(directory="templates")

# --- Global Objects ---
extractor = YouTubeAudioExtractor()
transcriber = SpeechTranscriber()

# Load summarization model at startup
print("ğŸ”„ ì•ˆì •í™”ëœ í•œêµ­ì–´ ìš”ì•½ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
try:
    # ì•ˆì •ì ì¸ T5 ëª¨ë¸ ì‚¬ìš© (ê¸´ í…ìŠ¤íŠ¸ ì²˜ë¦¬ ê°€ëŠ¥)
    summarizer = pipeline("summarization", 
                         model="eenzeenee/t5-small-korean-summarization",
                         max_length=1024,  # ìµœëŒ€ ê¸¸ì´ ì œí•œ
                         truncation=True)
    print("âœ… T5 í•œêµ­ì–´ ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì™„ë£Œ!")
except Exception as e:
    summarizer = None
    print(f"âŒ ìš”ì•½ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
    print("   ìš”ì•½ ê¸°ëŠ¥ ì—†ì´ ì• í”Œë¦¬ì¼€ì´ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤.")


# In-memory job store
jobs = {}

# --- Google Gemini ìš”ì•½ í•¨ìˆ˜ë“¤ ---
async def gemini_summarize_key_points(text: str):
    """Google Geminië¥¼ ì‚¬ìš©í•œ í•µì‹¬ìš”ì•½"""
    try:
        model = genai.GenerativeModel(GEMINI_MODEL)
        
        prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ í…ìŠ¤íŠ¸ ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ê° ë¬¸ë‹¨ì˜ í•µì‹¬ ë‚´ìš©ì„ ì •í™•í•˜ê³  ê°„ê²°í•˜ê²Œ ìš”ì•½í•´ì£¼ì„¸ìš”.

ì§€ì‹œì‚¬í•­:
1. ì£¼ì–´ì§„ í…ìŠ¤íŠ¸ë¥¼ ìì—°ìŠ¤ëŸ¬ìš´ ë¬¸ë‹¨ìœ¼ë¡œ ë‚˜ëˆ„ì„¸ìš”
2. ê° ë¬¸ë‹¨ì˜ í•µì‹¬ ë‚´ìš©ì„ 1-2 ë¬¸ì¥ìœ¼ë¡œ ìš”ì•½í•˜ì„¸ìš”
3. ê²°ê³¼ë¥¼ JSON ë°°ì—´ í˜•íƒœë¡œ ë°˜í™˜í•˜ì„¸ìš”
4. ê° í•­ëª©ì€ "paragraph_summary" í‚¤ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤

í…ìŠ¤íŠ¸:
{text}

ì‘ë‹µ í˜•ì‹ (JSONë§Œ ë°˜í™˜):
[
  {{"paragraph_summary": "ì²« ë²ˆì§¸ ë¬¸ë‹¨ ìš”ì•½"}},
  {{"paragraph_summary": "ë‘ ë²ˆì§¸ ë¬¸ë‹¨ ìš”ì•½"}},
  ...
]
"""
        
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.3,
                max_output_tokens=2000,
            )
        )
        
        result_text = response.text.strip()
        # JSON ì¶”ì¶œ (ì½”ë“œ ë¸”ë¡ ì œê±°)
        if result_text.startswith('```json'):
            result_text = result_text.split('```json')[1].split('```')[0].strip()
        elif result_text.startswith('```'):
            result_text = result_text.split('```')[1].split('```')[0].strip()
        
        return json.loads(result_text)
    except Exception as e:
        print(f"Gemini í•µì‹¬ìš”ì•½ ì˜¤ë¥˜: {e}")
        return None

async def gemini_summarize_curator(text: str):
    """Google Geminië¥¼ ì‚¬ìš©í•œ íë ˆì´í„° ìš”ì•½"""
    try:
        model = genai.GenerativeModel(GEMINI_MODEL)
        
        prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ ì½˜í…ì¸  íë ˆì´í„°ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ìœ íŠœë¸Œ ì˜ìƒ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ì‹œì²­ìê°€ 30ì´ˆ ì•ˆì— í•µì‹¬ì„ íŒŒì•…í•  ìˆ˜ ìˆë„ë¡ ìš”ì•½í•´ì£¼ì„¸ìš”.

ì§€ì‹œì‚¬í•­:
1. ì œëª©: ì˜ìƒ ë‚´ìš©ì„ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” ë§¤ë ¥ì ì¸ í•œ ë¬¸ì¥ ì œëª©
2. í•œ ì¤„ ìš”ì•½: ì „ì²´ ë‚´ìš©ì„ í•œ ë¬¸ì¥ìœ¼ë¡œ ì••ì¶•í•œ í•µì‹¬ ìš”ì•½
3. í•µì‹¬ í¬ì¸íŠ¸: ê°€ì¥ ì¤‘ìš”í•œ 3ê°œì˜ í¬ì¸íŠ¸

í…ìŠ¤íŠ¸:
{text}

ì‘ë‹µì„ JSON í˜•íƒœë¡œë§Œ ì œê³µí•´ì£¼ì„¸ìš”:
{{
  "title": "ë§¤ë ¥ì ì¸ ì œëª©",
  "one_line_summary": "ì „ì²´ ë‚´ìš©ì˜ í•µì‹¬ ìš”ì•½",
  "key_points": ["í¬ì¸íŠ¸ 1", "í¬ì¸íŠ¸ 2", "í¬ì¸íŠ¸ 3"]
}}
"""
        
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.4,
                max_output_tokens=1000,
            )
        )
        
        result_text = response.text.strip()
        # JSON ì¶”ì¶œ
        if result_text.startswith('```json'):
            result_text = result_text.split('```json')[1].split('```')[0].strip()
        elif result_text.startswith('```'):
            result_text = result_text.split('```')[1].split('```')[0].strip()
        
        return json.loads(result_text)
    except Exception as e:
        print(f"Gemini íë ˆì´í„° ìš”ì•½ ì˜¤ë¥˜: {e}")
        return None

async def gemini_summarize_timeline(text: str):
    """Google Geminië¥¼ ì‚¬ìš©í•œ íƒ€ì„ë¼ì¸ ìš”ì•½"""
    try:
        model = genai.GenerativeModel(GEMINI_MODEL)
        
        prompt = f"""
ë‹¹ì‹ ì€ ì „ë¬¸ ì˜ìƒ í¸ì§‘ìì´ì ìš”ì•½ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ìœ íŠœë¸Œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‹œê°„ íë¦„ì— ë”°ë¼ êµ¬ê°„ë³„ë¡œ ë‚˜ëˆ„ì–´ íƒ€ì„ë¼ì¸ í˜•íƒœë¡œ ì •ë¦¬í•´ì£¼ì„¸ìš”.

ì§€ì‹œì‚¬í•­:
1. ë‚´ìš©ì˜ íë¦„ì— ë”°ë¼ 4-8ê°œì˜ êµ¬ê°„ìœ¼ë¡œ ë‚˜ëˆ„ì„¸ìš”
2. ê° êµ¬ê°„ì— ëŒ€í•´ ë‹¤ìŒ ì •ë³´ë¥¼ ì œê³µí•˜ì„¸ìš”:
   - timestamp: "X-Yë¶„" í˜•íƒœì˜ ì‹œê°„ëŒ€
   - subtitle: í•´ë‹¹ êµ¬ê°„ì˜ í•µì‹¬ ì£¼ì œ (ê°„ê²°í•œ ì œëª©)
   - summary: êµ¬ê°„ ë‚´ìš©ì˜ ìƒì„¸ ìš”ì•½ (2-3ë¬¸ì¥)
   - keywords: í•µì‹¬ í‚¤ì›Œë“œ 3-5ê°œ
   - oneline_summary: êµ¬ì–´ì²´ë¡œ í•œ ë¬¸ì¥ ì •ë¦¬

í…ìŠ¤íŠ¸:
{text}

ì‘ë‹µì„ JSON ë°°ì—´ í˜•íƒœë¡œë§Œ ì œê³µí•´ì£¼ì„¸ìš”:
[
  {{
    "timestamp": "0-3ë¶„",
    "subtitle": "êµ¬ê°„ ì œëª©",
    "summary": "êµ¬ê°„ ë‚´ìš© ìš”ì•½",
    "keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
    "oneline_summary": "êµ¬ì–´ì²´ë¡œ í•œ ì¤„ ì •ë¦¬"
  }},
  ...
]
"""
        
        response = model.generate_content(
            prompt,
            generation_config=genai.types.GenerationConfig(
                temperature=0.5,
                max_output_tokens=2500,
            )
        )
        
        result_text = response.text.strip()
        # JSON ì¶”ì¶œ
        if result_text.startswith('```json'):
            result_text = result_text.split('```json')[1].split('```')[0].strip()
        elif result_text.startswith('```'):
            result_text = result_text.split('```')[1].split('```')[0].strip()
        
        return json.loads(result_text)
    except Exception as e:
        print(f"Gemini íƒ€ì„ë¼ì¸ ìš”ì•½ ì˜¤ë¥˜: {e}")
        return None

# --- Pydantic Models ---
class SummarizationRequest(BaseModel):
    text: str

# --- HTML Routes ---
@app.get("/", response_class=HTMLResponse)
async def home(request: Request):
    """ë©”ì¸ í˜ì´ì§€ - ê°œì„ ëœ ì ‘ê·¼ì„±ê³¼ ëª¨ë“ˆí™”ëœ ì»´í¬ë„ŒíŠ¸"""
    return templates.TemplateResponse("dashboard.html", {"request": request})

# --- API Routes ---
@app.post("/transcribe")
async def transcribe(
    background_tasks: BackgroundTasks,
    url: str = Form(...),
    format: str = Form("mp3"),
    method: str = Form("whisper"), 
    model: str = Form("base")
):
    """ìŒì„± ë³€í™˜ ì‘ì—… ì‹œì‘"""
    if 'youtube.com' not in url and 'youtu.be' not in url:
        raise HTTPException(status_code=400, detail="ìœ íš¨í•œ YouTube URLì„ ì…ë ¥í•´ì£¼ì„¸ìš”")
    
    job_id = str(uuid.uuid4())
    jobs[job_id] = {
        'status': 'ë¹„ë””ì˜¤ ì •ë³´ í™•ì¸ ì¤‘...', 
        'completed': False,
        'success': False,
        'result': None,
        'error': None,
        'created_at': datetime.now()
    }
    
    background_tasks.add_task(process_transcription, job_id, url, format, method, model)
    return {"job_id": job_id}


@app.get("/status/{job_id}")
async def get_status(job_id: str):
    """ì‘ì—… ìƒíƒœ í™•ì¸"""
    if job_id not in jobs:
        raise HTTPException(status_code=404, detail="ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
    return jobs[job_id]

@app.post("/summarize/key_summary")
async def summarize_key_points(payload: SummarizationRequest):
    """
    í•µì‹¬ìš”ì•½ API - OpenAI ìš°ì„ , T5 ë°±ì—…
    """
    # Google Gemini API ì‚¬ìš© (ìš°ì„ ìˆœìœ„)
    if USE_GEMINI_SUMMARY and GEMINI_API_KEY:
        try:
            result = await gemini_summarize_key_points(payload.text)
            if result:
                return JSONResponse(content=result)
        except Exception as e:
            print(f"Gemini í•µì‹¬ìš”ì•½ ì‹¤íŒ¨, T5ë¡œ ë°±ì—…: {e}")
    
    # T5 ëª¨ë¸ ë°±ì—…
    if not summarizer:
        # Use simple rule-based summarization if model is not available
        try:
            # Split by double newlines first, then by single newlines if no double newlines
            paragraphs = payload.text.split('\n\n')
            if len(paragraphs) == 1:
                paragraphs = [p.strip() for p in payload.text.split('\n') if p.strip() and len(p.strip()) > 10]
            else:
                paragraphs = [p.strip() for p in paragraphs if p.strip()]
            
            summaries = []
            for para in paragraphs:
                if len(para.strip()) < 30:  # ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨ì€ ê·¸ëƒ¥ ì‚¬ìš©
                    summary_text = para.strip()
                else:
                    # Simple extractive summarization: take first and most important sentences
                    sentences = [s.strip() for s in para.split('.') if s.strip()]
                    if len(sentences) <= 2:
                        summary_text = para.strip()
                    else:
                        # Take first sentence and longest sentence (likely contains key info)
                        first_sentence = sentences[0] + '.'
                        longest_sentence = max(sentences[1:], key=len, default='') + ('.' if sentences[1:] else '')
                        summary_text = first_sentence + (f' {longest_sentence}' if longest_sentence and longest_sentence != '.' else '')
                
                if summary_text and len(summary_text.strip()) > 5:
                    summaries.append({"paragraph_summary": summary_text.strip()})
            
            return JSONResponse(content=summaries)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    try:
        # Split by double newlines first, then by single newlines if needed
        paragraphs = payload.text.split('\n\n')
        if len(paragraphs) == 1:
            paragraphs = [p.strip() for p in payload.text.split('\n') if p.strip() and len(p.strip()) > 10]
        else:
            paragraphs = [p.strip() for p in paragraphs if p.strip()]
            
        summaries = []
        for para in paragraphs:
            if len(para) < 50:  # ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨ì€ ê·¸ëƒ¥ ì‚¬ìš©
                summary_text = para
            else:
                # í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (T5 ëª¨ë¸ ì•ˆì •ì„±)
                para_truncated = para[:800] if len(para) > 800 else para
                summary = summarizer(para_truncated, 
                                   max_length=120, 
                                   min_length=25, 
                                   do_sample=False,
                                   truncation=True)
                summary_text = summary[0]['summary_text']
            
            if summary_text and len(summary_text.strip()) > 5:
                summaries.append({"paragraph_summary": summary_text.strip()})
                
        return JSONResponse(content=summaries)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.post("/summarize/curator")
async def summarize_curator(payload: SummarizationRequest):
    """
    íë ˆì´í„° ìš”ì•½ API - OpenAI ìš°ì„ , T5 ë°±ì—…
    """
    # Google Gemini API ì‚¬ìš© (ìš°ì„ ìˆœìœ„)
    if USE_GEMINI_SUMMARY and GEMINI_API_KEY:
        try:
            result = await gemini_summarize_curator(payload.text)
            if result:
                return JSONResponse(content=result)
        except Exception as e:
            print(f"Gemini íë ˆì´í„° ìš”ì•½ ì‹¤íŒ¨, T5ë¡œ ë°±ì—…: {e}")
    
    # T5 ëª¨ë¸ ë°±ì—…
    if not summarizer:
        # Use simple rule-based approach if model is not available
        try:
            text = payload.text.strip()
            sentences = [s.strip() for s in text.split('.') if s.strip() and len(s.strip()) > 10]
            
            # Generate title from first meaningful sentence or first few words
            if sentences:
                title_candidate = sentences[0]
                if len(title_candidate) > 100:
                    # Take first part if too long
                    words = title_candidate.split()[:10]
                    title = ' '.join(words) + ('...' if len(words) == 10 else '')
                else:
                    title = title_candidate
            else:
                title = "YouTube ì˜ìƒ ìš”ì•½"
            
            # Generate one line summary from longest sentences
            if len(sentences) >= 2:
                summary_candidates = sorted(sentences[:5], key=len, reverse=True)[:2]
                one_line_summary = '. '.join(summary_candidates) + '.'
            else:
                one_line_summary = sentences[0] if sentences else "ì˜ìƒ ë‚´ìš© ìš”ì•½"
            
            # Extract key points from longest/most informative sentences
            if len(sentences) >= 3:
                key_point_candidates = sorted(sentences, key=len, reverse=True)[:3]
                key_points = [point + ('.' if not point.endswith('.') else '') for point in key_point_candidates]
            else:
                key_points = [s + ('.' if not s.endswith('.') else '') for s in sentences[:3]]
                
            # Ensure we have at least something
            if not key_points:
                key_points = ["ì˜ìƒì˜ ì£¼ìš” ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤."]

            curated_summary = {
                "title": title,
                "one_line_summary": one_line_summary,
                "key_points": key_points
            }
            return JSONResponse(content=curated_summary)
        except Exception as e:
            raise HTTPException(status_code=500, detail=f"íë ˆì´ì…˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")

    try:
        # T5 ëª¨ë¸ ì•ˆì •ì„±ì„ ìœ„í•œ í…ìŠ¤íŠ¸ ì œí•œ
        text_truncated = payload.text[:1000] if len(payload.text) > 1000 else payload.text
        full_summary = summarizer(text_truncated, 
                                max_length=200, 
                                min_length=50, 
                                do_sample=False,
                                truncation=True)[0]['summary_text']
        
        # ì œëª© ìƒì„±
        title_text = payload.text[:500]
        title = summarizer(title_text, 
                          max_length=60, 
                          min_length=15, 
                          do_sample=False,
                          truncation=True)[0]['summary_text']

        # í•µì‹¬ í¬ì¸íŠ¸ ì¶”ì¶œ - ê°œì„ ëœ ë°©ë²•
        sentences = [s.strip() for s in payload.text.split('.') if s.strip() and len(s.strip()) > 20]
        
        # Get meaningful sentences for key points
        if len(sentences) >= 3:
            # Sort by length and take diverse content
            key_point_candidates = sorted(sentences, key=len, reverse=True)[:6]
            # Select 3 most diverse points
            key_points = []
            for candidate in key_point_candidates:
                if len(key_points) >= 3:
                    break
                # Avoid very similar points
                is_similar = False
                for existing in key_points:
                    if len(set(candidate.split()) & set(existing.split())) > len(candidate.split()) * 0.5:
                        is_similar = True
                        break
                if not is_similar:
                    key_points.append(candidate + ('.' if not candidate.endswith('.') else ''))
        else:
            key_points = [s + ('.' if not s.endswith('.') else '') for s in sentences[:3]]
            
        # Ensure we have at least something
        if not key_points:
            key_points = ["ì˜ìƒì˜ ì£¼ìš” ë‚´ìš©ì„ ë‹¤ë£¹ë‹ˆë‹¤."]

        curated_summary = {
            "title": title,
            "one_line_summary": full_summary,
            "key_points": key_points
        }
        return JSONResponse(content=curated_summary)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íë ˆì´ì…˜ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.post("/summarize/timeline_summary")
async def summarize_timeline(payload: SummarizationRequest):
    """
    íƒ€ì„ë¼ì¸ ìš”ì•½ API - OpenAI ìš°ì„ , Rule-based ë°±ì—…
    """
    # Google Gemini API ì‚¬ìš© (ìš°ì„ ìˆœìœ„)
    if USE_GEMINI_SUMMARY and GEMINI_API_KEY:
        try:
            result = await gemini_summarize_timeline(payload.text)
            if result:
                return JSONResponse(content=result)
        except Exception as e:
            print(f"Gemini íƒ€ì„ë¼ì¸ ìš”ì•½ ì‹¤íŒ¨, Rule-basedë¡œ ë°±ì—…: {e}")
    
    # Rule-based ë°±ì—…
    try:
        text = payload.text.strip()
        if not text or len(text) < 20:
            return JSONResponse(content=[])
        
        # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ë‹¨ìœ¼ë¡œ ë¶„í•  (ë” ê¸´ ë¬¸ë‹¨ìœ¼ë¡œ)
        paragraphs = []
        sentences = [s.strip() for s in text.split('.') if s.strip() and len(s.strip()) > 15]
        
        # ë¬¸ì¥ë“¤ì„ ê·¸ë£¹í™”í•˜ì—¬ ë‹¨ë½ ìƒì„± (ì•½ 3-5 ë¬¸ì¥ì”©)
        current_paragraph = []
        for sentence in sentences:
            current_paragraph.append(sentence)
            # ë¬¸ë‹¨ ê¸¸ì´ê°€ ì ë‹¹í•˜ê±°ë‚˜ í‚¤ì›Œë“œ ë³€í™”ê°€ ê°ì§€ë˜ë©´ ìƒˆ ë¬¸ë‹¨ ì‹œì‘
            if len(current_paragraph) >= 4 or len(' '.join(current_paragraph)) > 300:
                paragraphs.append('. '.join(current_paragraph) + '.')
                current_paragraph = []
        
        # ë‚¨ì€ ë¬¸ì¥ë“¤ ì¶”ê°€
        if current_paragraph:
            paragraphs.append('. '.join(current_paragraph) + '.')
        
        timeline_sections = []
        
        for i, paragraph in enumerate(paragraphs):
            if len(paragraph.strip()) < 20:  # ë„ˆë¬´ ì§§ì€ ë¬¸ë‹¨ ì œì™¸
                continue
                
            # íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (ëŒ€ëµì ìœ¼ë¡œ)
            timestamp = f"{(i * 3) + 1}-{(i + 1) * 3}ë¶„"
            
            # ì†Œíƒ€ì´í‹€ ìƒì„± (ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ í•µì‹¬ ì¶”ì¶œ)
            first_sentence = paragraph.split('.')[0].strip()
            if len(first_sentence) > 60:
                words = first_sentence.split()[:8]
                subtitle = ' '.join(words) + '...'
            else:
                subtitle = first_sentence
            
            # ë‹¨ë½ ìš”ì•½ (í•µì‹¬ ë¬¸ì¥ë“¤ ì„ ë³„)
            sentences_in_paragraph = [s.strip() for s in paragraph.split('.') if s.strip()]
            if len(sentences_in_paragraph) > 1:
                # ê°€ì¥ ê¸´ 2-3ê°œ ë¬¸ì¥ì„ í•µì‹¬ìœ¼ë¡œ ì„ íƒ
                important_sentences = sorted(sentences_in_paragraph[:4], key=len, reverse=True)[:2]
                summary = '. '.join(important_sentences) + '.'
            else:
                summary = paragraph
            
            # í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ)
            words = paragraph.lower().split()
            # ì¼ë°˜ì ì¸ ë¶ˆìš©ì–´ ì œê±° í›„ ë¹ˆë„ ë†’ì€ ë‹¨ì–´ë“¤ì„ í‚¤ì›Œë“œë¡œ
            stop_words = {'ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì˜', 'ì—', 'ì—ì„œ', 'ìœ¼ë¡œ', 'ì™€', 'ê³¼', 'í•˜ê³ ', 'ê·¸ë¦¬ê³ ', 'ë˜í•œ', 'í•˜ì§€ë§Œ', 'ê·¸ëŸ°ë°', 'ê·¸ë˜ì„œ', 'ë”°ë¼ì„œ', 'ì¦‰', 'ê²ƒ', 'ìˆ˜', 'ë•Œ', 'ê³³', 'ë“±'}
            meaningful_words = [w for w in words if len(w) > 1 and w not in stop_words]
            word_freq = {}
            for word in meaningful_words:
                word_freq[word] = word_freq.get(word, 0) + 1
            
            # ë¹ˆë„ìˆ˜ ê¸°ì¤€ ìƒìœ„ 3-5ê°œ í‚¤ì›Œë“œ ì„ íƒ
            keywords = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)[:4]
            keyword_list = [kw[0] for kw in keywords if kw[1] > 1]
            
            # í•œë§ˆë”” ì •ë¦¬ (êµ¬ì–´ì²´ë¡œ)
            if len(sentences_in_paragraph) > 0:
                key_sentence = sentences_in_paragraph[0]
                if "í•©ë‹ˆë‹¤" in key_sentence or "ìŠµë‹ˆë‹¤" in key_sentence:
                    oneline = key_sentence.replace("í•©ë‹ˆë‹¤", "í•´ìš”").replace("ìŠµë‹ˆë‹¤", "ì˜ˆìš”")
                else:
                    oneline = key_sentence + "ë¼ëŠ” ì–˜ê¸°ì—ìš”"
            else:
                oneline = "í•µì‹¬ ë‚´ìš©ì´ì—ìš”"
            
            timeline_sections.append({
                "timestamp": timestamp,
                "subtitle": subtitle,
                "summary": summary,
                "keywords": keyword_list,
                "oneline_summary": oneline
            })
        
        return JSONResponse(content=timeline_sections[:8])  # ìµœëŒ€ 8ê°œ ì„¹ì…˜ìœ¼ë¡œ ì œí•œ
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"íƒ€ì„ë¼ì¸ ìš”ì•½ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


# --- Background Task ---
async def process_transcription(job_id: str, url: str, format: str, method: str, model: str):
    """ë°±ê·¸ë¼ìš´ë“œ ìŒì„± ë³€í™˜ ì²˜ë¦¬"""
    try:
        jobs[job_id]['status'] = 'ë¹„ë””ì˜¤ ì •ë³´ í™•ì¸ ì¤‘...'
        video_info = extractor.get_video_info(url)
        if not video_info:
            raise Exception('ë¹„ë””ì˜¤ ì •ë³´ë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤')

        jobs[job_id]['status'] = f'ì˜¤ë””ì˜¤ ì¶”ì¶œ ì¤‘... ({format})'
        audio_path = extractor.extract_audio(url, format)
        if not audio_path:
            raise Exception('ì˜¤ë””ì˜¤ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤')

        jobs[job_id]['status'] = f'ìŒì„± ì¸ì‹ ì¤‘... ({method})'
        transcriber_instance = SpeechTranscriber(model_name=model)
        result = transcriber_instance.transcribe(audio_path, method=method)
        
        if os.path.exists(audio_path):
            os.remove(audio_path)
        
        if result.get('success', False):
            jobs[job_id].update({
                'status': 'ë³€í™˜ ì™„ë£Œ!',
                'completed': True,
                'success': True,
                'result': result
            })
        else:
            raise Exception(result.get('error', 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜'))
            
    except Exception as e:
        jobs[job_id].update({
            'completed': True,
            'success': False,
            'error': str(e)
        })

# --- Main Execution ---
if __name__ == "__main__":
    print("ğŸš€ YouTube Audio Transcriber ì›¹ ì„œë²„ ì‹œì‘")
    print("ğŸ“± http://localhost:8001 ì—ì„œ ì ‘ì† ê°€ëŠ¥")
    uvicorn.run(app, host="0.0.0.0", port=8001)